<h1 align="left">Speakers</h1>
<br>

<div id="about" class="panel">
    <ul class="publications">
        <li><a href="https://www.nordita.org/people/staff/index.php?u=soon.hoe.lim">Soon Hoe Lim, Nordita</a></li>
        <li><a href="https://ytian.netlify.app/">Yu Tian, Nordita</a></li>
        <li><a href="https://www.nordita.org/people/staff/index.php?u=niccolo.zagli">Niccolò Zagli, Nordita</a></li>
        <li><a href="https://www.kth.se/profile/uribarri">Gonzalo Uribarri, KTH</a></li>
        <li><a href="https://staff.ki.se/people/mite-mijalkov">Mite Mijalkov, Karolinska Institutet</a></li>
        <li><a href="https://staff.ki.se/people/julia-ericson">Julia Ericson, Karolinska Institutet</a></li>
        <li><a href="https://twitter.com/DavidBeersMath">David Beers, University of Oxford</a></li>
        <li><a href="https://people.maths.ox.ac.uk/sumray/">Otto Sumray, University of Oxford</a></li>
        <li><a href="https://danielaegassan.github.io/">Daniela Egas Santander, Blue Brain Project</a></li>
        <li><a href="https://www.ncl.ac.uk/computing/staff/profile/petertaylor.html">Peter Taylor, Newcastle University</a></li>
        <li><a href="https://www.fz-juelich.de/profile/tiberi_l">Lorenzo Tiberi, Forschungszentrum Jülich</a></li>
        <li><a href="https://christineahrends.org/">Christine Ahrends, Aarhus University</a></li>
        <li><a href="http://www.rdgao.com/">Richard Gao, University of Tübingen</a></li>
        <li><a href="https://victorseven.github.io/about/">Victor Buendi&#769;a, University of Tübingen</a></li>
    </ul>
</div>

<br><br>

<h1 align="left">Abstracts</h1>
<br>

<div id="about" class="panel">
    <b>Victor Buendi&#769;a: Critical synchronization in brain networks</b><br>
    Collective oscillations are one of most prominent phenomena at the large scale in the brain. Many efforts have been dedicated to understand how collective oscillations emerge in networks, and how brain topology constraints such phenomena. Additionally, several studies have found that critical synchronization transitions might play a role in neuronal dynamics. In this tutorial, I will review the basics of synchronization models, and how to construct phase diagrams from them both numerically and analytically. Then, I will show how these results are affected by the topology of hierarchical-modular networks such as the ones found in the brain. To finish, I will discuss the possibility of using fMRI data to determine model's parameters and discuss the "criticality hypothesis" at the large scale.
</div>
<br>
<div id="about" class="panel">
    <b>Lorenzo Tiberi: Hidden synaptic structures control collective network dynamics</b><br>
    A common approach to model local neural circuits is to assume random connectivity. But how is our choice of randomness informed by known network properties? And how does it affect the network's behavior? Previous approaches have focused on prescribing increasingly sophisticated statistics of synaptic strengths and motifs. However, at the same time experimental data on parallel dynamics of neurons is readily accessible. I therefore propose a complementary approach, specifying connectivity in the space that directly controls the dynamics - the space of eigenmodes. I will expose a recently developed theory for a novel ensemble of large random matrices, whose eigenvalue distribution can be chosen arbitrarily. I will show how varying such distribution induces a diverse range of collective network behaviors, including power laws that characterize the dimensionality, principal components spectrum, autocorrelation, and autoresponse of neuronal activity. The power-law exponents are controlled by the density of nearly critical eigenvalues, and provide a minimal and robust measure to directly link observable neuronal dynamics and connectivity. I will also show that the wide range of dynamical behaviors resulting from the proposed connectivity ensemble is caused by structures that are invisible to a motif analysis. Their presence is captured by motifs appearing with vanishingly small probability in the number of neurons. Only reciprocal motifs occur with finite probability. This result shows that a motif analysis can be blind to synaptic structures controlling the dynamics, which instead become apparent in the eigenmode statistics.
</div>
<br>
<div id="about" class="panel">
    <b>Niccolo&#768; Zagli: Response Theory, Reaction Coordinates and Critical Phenomena in noisy interacting systems</b><br>
    In this talk I will present our latest results on the close link between response theory, reaction coordinates and critical phenomena for noisy systems with mean field interactions. Such systems are routinely used to model collective emergent behaviours in multiple areas of social and natural sciences as they exhibit, in the thermodynamic limit, continuous and discontinuous phase transitions. Firstly, I will show how to build a framework to identify suitable reaction coordinates for the system starting from the microscopic properties of the interaction structure among the agents, stating conditions on the interactions that lead to the identification of a finite number of reaction coordinates and a consequent dimension reduction of the system. Secondly, I will show that such reaction coordinates prove to be proper nonequilibrium thermodynamic variables as they carry information on correlation and memory properties and provide the natural way to probe the sensitivity of the system to external perturbations. In particular, the investigation of response properties of the reaction coordinates allows to identify, pinpoint and characterise phase transitions of the system as they manifest as singular values of the susceptibility associated to such thermodynamic variables. The investigation of static and dynamic phase transitions for the Kuramoto model will be considered as a paradigmatic example of our theory.
</div>
<br>
<div id="about" class="panel">
    <b>Peter Taylor: Detecting abnormalities in epileptogenic networks</b><br>
    Over 60 million people worldwide have epilepsy, and around a third are refractory to medication.  Neurosurgery, where the epileptogenic tissue is removed, is a major alternative to medication and can be curative for some. Unfortunately, around half of patients still experience seizures even after invasive neurosurgery. The key to successful neurosurgery is localisation of the part of the brain causing the seizures. This can be straightforward when there is a clear abnormal lesion visible on MRI. However, more subtle abnormalities can be difficult to see, and advanced computational approaches are required. In this talk I will present approaches to identify brain abnormalities to predict patient outcomes. Predictions are validated with post-operative imaging to delineate resections, and actual patient outcomes. I will show how abnormalities in different imaging modalities (EEG, intracranial EEG, MEG, dMRI, T1w-MRI) can be quantified, and I will suggest strategies for multimodal data integration. Finally, I will demonstrate our novel software tool to visualise these abnormalities for prospective clinical application.
</div>
<br>
<div id="about" class="panel">
    <b>Julia Ericson: Functional connectivity in Magnetoencephalography</b><br>
    Functional connectivity refers to the correlation of activity between brain regions, which can be investigated at a high temporal resolution using Magneto- (MEG) and electroencephalography (EEG). The high temporal resolution allows us to study correlations between collective neural oscillations as well as the coupling of different frequencies. However, a major problem when measuring and analyzing functional connectivity using M/EEG is field spread and volume conductance. This leads to source mixing and artifactual correlations between brain regions. In this talk, I will discuss ways to tackle these issues, both when projecting the activity recorded by the sensors to the source space of the brain and when constructing connectivity measures. Finally, I will give examples of how these methods are used in network neuroscience, especially to study working memory, and how functional connectivity relates to neural mechanisms.
</div>
<br>
<div id="about" class="panel">
    <b>Christine Ahrends: Describing and predicting from transient patterns of brain function using Hidden Markov Models</b><br>
    Whole-brain networks of brain function fluctuate dynamically, enabling flexible neuronal communication and functioning across the entire brain. The Hidden Markov Model (HMM) is a powerful and versatile tool to describe these recurrent, transient patterns of brain function over time, e.g., in terms of amplitude, functional connectivity, and/or frequency. This model allows reducing complex brain dynamics into a subset of patterns of transitions between quasi-stationary states. I will discuss practical questions, like how HMMs can be implemented in different neuroimaging modalities such as fMRI or MEG, how we can improve their reliability, and how we can evaluate them using simulated data or null models. Finally, I will show how we can use an HMM describing transient patterns of brain function to predict individual subject traits in a mathematically principled way using kernel methods.
</div>
<br>
<div id="about" class="panel">
    <b>Gonzalo Uribarri: Detection of Parkinson's disease features in MEG recordings</b><br>
    The talk will focus on the use of machine learning and dynamical systems techniques on MEG data to find features that distinguish Parkinson's disease patients from healthy controls. I will introduce the techniques, present results on experimental data, and discuss the importance of explainability in the context of medical research.
</div>
<br>
<div id="about" class="panel">
    <b>Daniela Egas Santander: Directionality in brain networks</b><br>
    A strong hypothesis in neuroscience is that many aspects of brain function are determined by the ``map of the brain" and that its computational power relies on its connectivity architecture. Impressive scientific and engineering advances in recent years generated a plethora of large brain networks of incredibly complex architectures. A crucial aspect of the architecture is its inherent directionality reflecting the direction of information flow. Two of the stark differences between directed and undirected networks is the presence of reciprocal connections and cliques of neurons with different levels of directionality. It has been shown that both reciprocal connections and directed cliques (those that maximize directionality) are overrepresented motifs in neural networks and that these are formed selectively rather than randomly. This brings forward the questions: how do these motifs interact with each other? what is their function? and how to build appropriate null-models that take this into account? We explore these questions from the perspective of mathematics and of computational neuroscience.
</div>
<br>
<div id="about" class="panel">
    <b>Soon Hoe Lim: Noisy Recurrent Neural Networks</b><br>
    In this talk, we discuss a general framework for studying recurrent neural networks (RNNs) trained by injecting noise into hidden states. Specifically, we consider RNNs that can be viewed as discretizations of stochastic differential equations driven by input data. This framework allows us to study the implicit regularization effect of general noise injection schemes by deriving an approximate explicit regularizer in the small noise regime. We find that, under reasonable assumptions, this implicit regularization promotes flatter minima; it biases towards models with more stable dynamics; and, in classification tasks, it favors models with larger classification margin. Our theory is supported by empirical results which demonstrate that the RNNs have improved robustness with respect to various input perturbations.
</div>
<br>
<div id="about" class="panel">
    <b>Yu Tian: Spreading and Structural Balance on Signed Networks</b><br>
    Two competing types of interactions often play an important part in shaping system behavior such as activatory or inhibitory functions in biological systems. Hence, signed networks, where each connection can be either positive or negative, have become popular models recently. However, the primary focus of the literature is on the unweighted and structurally balanced ones, where all cycles have an even number of edges. Hence here, we first introduce a classification of signed networks into balanced, antibalanced or strictly balanced ones, and then characterize each type of signed networks in terms of the spectral properties of the signed weighted adjacency matrix. These properties are important to understand the dynamics on signed networks, both linear and nonlinear ones. Specifically, we find consistent patterns in a linear and a nonlinear dynamics theoretically, depending on their type of balance. We also propose two measures to further characterize strictly unbalanced networks, motivated by perturbation theory. Finally, we numerically verify these properties through experiments on both synthetic and real networks.
</div>
<br>
<div id="about" class="panel">
    <b>Richard Gao: Linking population dynamics to circuit mechanisms: analysis, synthesis, and inference</b><br>
    Variations in cellular and network parameters shape neural dynamics, which in turn form the basis of neural computation and cognition. Inferring those biological mechanisms from readouts of neural dynamics can be framed as the inverse problem. In my talk, I will highlight how recent advances in neural spectra parameterization (analysis) and mechanistic modeling (synthesis) form complementary efforts towards the inverse problem. First, I will introduce spectral parameterization (FOOOF) as a tool to potentially derive more biologically meaningful features in power spectra of neurophysiological time series, with examples to demonstrate how data-driven analysis in terms of periodic and aperiodic neural activity can lead to discovery of novel biomarkers. Second, I will talk about mechanistic modeling of neural circuits, and in particular, spiking neural network simulations, and how recent advances in simulation-based inference (SBI) and deep neural density estimators, can help us discover data-consistent parameters for such mechanistic models. Together, I want to argue for a tighter link between analysis, simulations, and experimental data across model organisms, and raise interesting research questions that can arise as a result.
</div>
<br>
<div id="about" class="panel">
    <b>Otto Sumray: Networks in single-cell data: new multiscale methods for selecting gene expression signals</b><br>
    Analysis of single-cell transcriptomics often relies on clustering cells and then performing differential gene expression to identify genes that vary between these clusters. These discrete analyses successfully determine cell types and markers; however, continuous variation within and between cell types may not be detected. In network science, the Laplacian score is used for feature selection on networks by measuring variation in graph signals with respect to the graph. However, in practice different interesting signals in data may have variation on different scales and location but will not be distinguished by the Laplacian score. We present three multiscale extensions of the Laplacian score for ranking and visualisation of graph signals. Eigenscores use the eigendecomposition of the graph Laplacian; the multiscale Laplacian score uses continuous random walks; the persistent Rayleigh quotient uses the Kron reduced or persistent Laplacian and a given filtration. Our methods find genes that exhibit smooth variation with and between known cell types that are not picked out by traditional cluster-based methods and allow informative visualisation of the space of genes.
</div>
<br>
<div id="about" class="panel">
    <b>David Beers: Topological methods for the brain: from single-cell to the connectome</b><br>
    We present results for two topics in mathematical neuroscience. The first of these topics concerns neurons, whose morphology is important for their function and therefore is often used to classify them. The topological morphology descriptor, a recent method developed to compare neuronal geometry, assigns a morphology descriptor called a barcode to a neuron equipped with a given function, such as the path distance or Euclidean distance from the root of the neuron. These barcodes can be converted into matrices called persistence images, which can then be averaged across groups and provide an interpretable summary of morphology. We show that persistence images are stable against perturbations of input data, and introduce topological morphology functions, a class of functions similar to Sholl functions, that can be recovered from the associated topological morphology descriptor. To demonstrate this topological approach, we compare healthy cortical and hippocampal mouse neurons to those affected by progressive tauopathy. We find a difference in the morphology of healthy neurons and those with a tauopathy at a postsymptomatic age. We use persistence images to conclude that the diseased group tends to have neurons with shorter branches as well as fewer branches far from the soma. Our second topic concerns sequences of macroscale brain networks whose accumulating edges represent the degeneration of connections in a human brain due to neurodegenerative disease. These sequences are viewed as chains in the poset of graphs on N nodes up to homotopy, where the partial order indicates when one graph can be attained from another by attaching edges. We attain rough scaling laws for the size of this poset as N grows by connecting the combinatorics of graph homotopy types with the notion of partitions from number theory. Then we observe that chains in this poset are able to distinguish different subtypes of simulated Alzheimer's disease.
</div>
<br>
<div id="about" class="panel">
    <b>Mite Mijalkov: Building and analyzing complex networks of the brain: Insights from neurodegenerative disorders and healthy aging</b><br>
    Complex networks have emerged as a powerful method for exploring the structure and function of the brain. In this talk, I will introduce the concept of complex networks and how they can be applied to model the brain. I will also cover various techniques for calculating whole-brain networks using structural and functional imaging data, as well as discuss how to use the framework of multilayer networks to integrate information about the different properties of the network. Throughout my talk, I will show previous studies that have applied these methods to provide novel insights into several neurodegenerative disorders, including Alzheimer's and Parkinson's disease, as well as healthy aging. Specifically, I will demonstrate that the changes in connectivity patterns in the brain correspond to disease severity and individual cognitive abilities, highlighting their potential and utility as biomarkers for early detection and diagnosis of neurodegenerative disorders. Overall, this talk aims to provide a comprehensive overview of the exciting opportunities and challenges associated with using complex network analysis to explore the brain and its diseases, and how these techniques can contribute to a deeper understanding of brain structure and function.
</div>
