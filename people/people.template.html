<h1 align="left">Speakers</h1>
<br>

<div id="about" class="panel">
    <ul class="publications">
        <li><a href="https://www.nordita.org/people/staff/index.php?u=soon.hoe.lim">Soon Hoe Lim, Nordita</a></li>
        <li><a href="https://ytian.netlify.app/">Yu Tian, Nordita</a></li>
        <li><a href="https://www.nordita.org/people/staff/index.php?u=niccolo.zagli">Niccolò Zagli, Nordita</a></li>
        <li><a href="https://www.kth.se/profile/uribarri">Gonzalo Uribarri, KTH</a></li>
        <li><a href="https://staff.ki.se/people/mite-mijalkov">Mite Mijalkov, Karolinska Institutet</a></li>
        <li><a href="https://staff.ki.se/people/julia-ericson">Julia Ericson, Karolinska Institutet</a></li>
        <li><a href="https://twitter.com/DavidBeersMath">David Beers, Oxford University</a></li>
        <li><a href="https://danielaegassan.github.io/">Daniela Egas Santander, Blue Brain Project</a></li>
        <li><a href="https://www.ncl.ac.uk/computing/staff/profile/petertaylor.html">Peter Taylor, Newcastle University</a></li>
        <li><a href="https://www.fz-juelich.de/profile/tiberi_l">Lorenzo Tiberi, Forschungszentrum Jülich</a></li>
        <li><a href="https://christineahrends.org/">Christine Ahrends, Aarhus University</a></li>
        <li><a href="http://www.rdgao.com/">Richard Gao, University of Tübingen</a></li>
        <li><a href="https://victorseven.github.io/about/">Victor Buendia, University of Tübingen</a></li>
    </ul>
</div>

<br><br>

<h1 align="left">Abstracts</h1>
<br>

<div id="about" class="panel">
    <b>Soon Hoe Lim: Noisy Recurrent Neural Networks</b><br>
    In this talk, we discuss a general framework for studying recurrent neural networks (RNNs) trained by injecting noise into hidden states. Specifically, we consider RNNs that can be viewed as discretizations of stochastic differential equations driven by input data. This framework allows us to study the implicit regularization effect of general noise injection schemes by deriving an approximate explicit regularizer in the small noise regime. We find that, under reasonable assumptions, this implicit regularization promotes flatter minima; it biases towards models with more stable dynamics; and, in classification tasks, it favors models with larger classification margin. Our theory is supported by empirical results which demonstrate that the RNNs have improved robustness with respect to various input perturbations.
</div>
<br>
<div id="about" class="panel">
    <b>Victor Buendia: Critical synchronization in brain networks</b><br>
    Collective oscillations are one of most prominent phenomena at the large scale in the brain. Many efforts have been dedicated to understand how collective oscillations emerge in networks, and how brain topology constraints such phenomena. Additionally, several studies have found that critical synchronization transitions might play a role in neuronal dynamics. In this tutorial, I will review the basics of synchronization models, and how to construct phase diagrams from them both numerically and analytically. Then, I will show how these results are affected by the topology of hierarchical-modular networks such as the ones found in the brain. To finish, I will discuss the possibility of using fMRI data to determine model's parameters and discuss the "criticality hypothesis" at the large scale.
</div>
<br>
<div id="about" class="panel">
    <b>Richard Gao: Linking population dynamics to circuit mechanisms: analysis, synthesis, and inference</b><br>
    Variations in cellular and network parameters shape neural dynamics, which in turn form the basis of neural computation and cognition. Inferring those biological mechanisms from readouts of neural dynamics can be framed as the inverse problem. In my talk, I will highlight how recent advances in neural spectra parameterization (analysis) and mechanistic modeling (synthesis) form complementary efforts towards the inverse problem. First, I will introduce spectral parameterization (FOOOF) as a tool to potentially derive more biologically meaningful features in power spectra of neurophysiological time series, with examples to demonstrate how data-driven analysis in terms of periodic and aperiodic neural activity can lead to discovery of novel biomarkers. Second, I will talk about mechanistic modeling of neural circuits, and in particular, spiking neural network simulations, and how recent advances in simulation-based inference (SBI) and deep neural density estimators, can help us discover data-consistent parameters for such mechanistic models. Together, I want to argue for a tighter link between analysis, simulations, and experimental data across model organisms, and raise interesting research questions that can arise as a result.
</div>
