<h1 align="left">Speakers and Abstracts</h1>
<br>

<div id="about" class="panel">
    <ul class="publications">
        <li><a href="https://www.nordita.org/people/staff/index.php?u=soon.hoe.lim">Soon Hoe Lim, Nordita</a></li>
        <li><a href="https://ytian.netlify.app/">Yu Tian, Nordita</a></li>
        <li><a href="https://www.nordita.org/people/staff/index.php?u=niccolo.zagli">Niccolò Zagli, Nordita</a></li>
        <li><a href="https://www.kth.se/profile/uribarri">Gonzalo Uribarri, KTH</a></li>
        <li><a href="https://staff.ki.se/people/mite-mijalkov">Mite Mijalkov, Karolinska Institutet</a></li>
        <li><a href="https://staff.ki.se/people/julia-ericson">Julia Ericson, Karolinska Institutet</a></li>
        <li><a href="https://twitter.com/DavidBeersMath">David Beers, Oxford University</a></li>
        <li><a href="https://danielaegassan.github.io/">Daniela Egas Santander, Blue Brain Project</a></li>
        <li><a href="https://www.ncl.ac.uk/computing/staff/profile/petertaylor.html">Peter Taylor, Newcastle University</a></li>
        <li><a href="https://www.fz-juelich.de/profile/tiberi_l">Lorenzo Tiberi, Forschungszentrum Jülich</a></li>
        <li><a href="https://christineahrends.org/">Christine Ahrends, Aarhus University</a></li>
        <li><a href="http://www.rdgao.com/">Richard Gao, University of Tübingen</a></li>
        <li><a href="https://victorseven.github.io/about/">Victor Buendia, University of Tübingen</a></li>
    </ul>
</div>

<h1 align="left">Speakers and Abstracts</h1>
<br>

<div id="about" class="panel">
    <b>Soon Hoe Lim: Noisy Recurrent Neural Networks</b><br>
    In this talk, we discuss a general framework for studying recurrent neural networks (RNNs) trained by injecting noise into hidden states. Specifically, we consider RNNs that can be viewed as discretizations of stochastic differential equations driven by input data. This framework allows us to study the implicit regularization effect of general noise injection schemes by deriving an approximate explicit regularizer in the small noise regime. We find that, under reasonable assumptions, this implicit regularization promotes flatter minima; it biases towards models with more stable dynamics; and, in classification tasks, it favors models with larger classification margin. Our theory is supported by empirical results which demonstrate that the RNNs have improved robustness with respect to various input perturbations.
</div>
